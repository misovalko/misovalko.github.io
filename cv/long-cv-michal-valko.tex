\documentclass{resume}

%%%%
\usepackage{breakurl} 
\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{blued}{RGB}{70,197,221}
\definecolor{pearOne}{HTML}{2C3E50}
%A9CF54
\definecolor{pearTwo}{HTML}{A9CF54}
\definecolor{pearTwoT}{HTML}{C2895B}
\definecolor{pearThree}{HTML}{E74C3C}
\colorlet{titleTh}{pearOne}
\colorlet{bull}{pearTwo}
\definecolor{pearcomp}{HTML}{B97E29}
\definecolor{pearFour}{HTML}{588F27}
\definecolor{pearFith}{HTML}{ECF0F1}
\definecolor{pearDark}{HTML}{2980B9}
\definecolor{pearDarker}{HTML}{1D2DEC}
\hypersetup{
	colorlinks,
	citecolor=pearDark,
	linkcolor=pearThree,
	urlcolor=pearDarker}
%%%%%%%%%%	
	
\input{resume_header}
\begin{document}
\maketitle

% \begin{category}{Objective}
% \citemnobullet 
%  Seeking a position in which I can continue doing research in \emph{machine learning}.
% \end{category}


% ------- Education ---------------------------------------------------

\begin{category}{Experience}
\citem{\href{https://deepmind.com}{DeepMind}}, Paris, France, \\
Senior Staff Research Scientist (2022 -- \dots) \\
Staff Research Scientist (2019 -- 2021) 
\citem{\href{http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/}{ENS Paris-Saclay -- Master 2 MVA}}, Paris-Saclay, France \\
External Lecturer - CEV (2014 -- $\dots$)
\citem{\href{https://team.inria.fr/sequel/}{Inria -- team SequeL}}, Lille, France \\
Experienced Junior Scientist - CR1/CRCN (2013 -- $\dots$) \\
External Collaborator - CR1/CRCN (2019 -- $\dots$) 
\citem{Inria -- team SequeL}, Lille, France \\
Junior Scientist - CR2 (2012 -- 2013)
\citem{Inria -- team SequeL}, Lille, France \\
Postdoctoral Researcher (2011 -- 2012), Advisor:  \emph{R\'emi Munos}
\citem{Intel Research},  Santa Clara, CA, USA\\
Research Intern (2009, 2010), Advisor:  \emph{Branislav Kveton}
\begin{category}{Teaching}
\citem{\href{http://researchers.lille.inria.fr/~valko/hp/mva-ml-graphs.php}{Graphs in machine learning}},  %(GPA 4.0) \\
\' Ecole normale sup\' erieure de Paris-Saclay, Cachan, France, 
Since 2013/2014,  graduate course $\approx$ 60--90 students
\end{category}
\end{category}
\begin{category}{Education}
\citem{\' Ecole normale sup\' erieure de Paris-Saclay}, Paris-Saclay, France \\ %(GPA 4.0) \\
HdR in Mathematics, June 2016.\\
Thesis: \href{http://researchers.lille.inria.fr/~valko/hp/publications/valko2016bandits.pdf}{\emph{Bandits on Graphs and Structures}}, Jury:\/ \emph{Nicolas Vayatis, Aur\' elien Garivier, Vianney Perchet, Nicol\`o Cesa-Bianchi, G\' abor Lugosi,  Mark Herbster, R\' emi Munos}
\citem{University of Pittsburgh}, Pittsburgh, PA \\ %(GPA 4.0) \\
PhD in Machine Learning, August 2011.\\
Thesis: \emph{Adaptive Graph-Based Algorithms}, Advisor: \emph{Milos Hauskrecht}
\citem{Comenius University Bratislava}, Slovakia\\% (GPA 3.96) \\
MSc., summa cum laude in Computer Science, June 2005.\\
Majors: Artificial Intelligence  and Mathematical Methods of CS\\ %(GPA 4.0) and (GPA 4.0)
Thesis: \emph{Evolving Neural Networks for Statistical Decision Theory}, Advisor:  \emph{R.\@Harman}
\end{category}
\begin{category}{Selected Awards}
	\citemnobullet Inria award for scientific excellence:
Prime d'excellence scientifique (2018 - 2021)
	\citemnobullet Inria award for scientific excellence:
Prime d'excellence scientifique (2014 - 2017)
\citemnobullet International Conference on Machine Learning \emph{Top 10} Reviewer Award (2018)
\citemnobullet International Conference on Machine Learning Reviewer Award (2015)
\citemnobullet Distinguished Alumni of Comenius University, Slovakia (2015)
\citemnobullet  Google best paper of online learning in vision (2010)
\citemnobullet Homer Warner Award by AMIA and OMG (2010)
	\citemnobullet Compunetix Best Research Award at Computer Science 
Department (2008 and 2011)	
	\citemnobullet University of Pittsburgh Honors Convocation Recognition (2009)
       \citemnobullet Andrew Mellon Predoctoral Fellowship (Fall 2008, Summer 
2009)	
       \citemnobullet Slovak Academy of Sciences Fellowship (2003 -- 2005)

\end{category}
	%\item Intel Labs Internship (Summer 2010)
	%\item NeurIPS Conference Travel Award (Fall 2009)
	%\item Intel Research Internship (Spring 2009)
	%\item Research Assistantship (Spring 2006 -- Spring 2008)
	%\item Teaching Assistantship (Fall 2005)
	%\item European Erasmus Scholarship, Lisbon, Portugal (Spring 2005)

% --------- Research ----------------------------------------------------

\begin{category}{Research Interests}
\citemnobullet machine learning, bandit theory, minimal feedback, 
online learning, sequential learning, graph-based methods, inverse 
reinforcement learning, semi-supervised learning
\end{category}


% -------- Collaborative Projects --------------------------------------------
\begin{category}{Project Funding}
\citembullet \href{https://www.upf.edu/web/delta}{DELTA} (EU CHIST-ERA), 2018 - 2022  (Project Coordinator, PI: A.\,Jonsson)
\citembullet  PGMO-IRMO grant of Fondation Math\' ematique Jacques Hadamard: Theoretically grounded efficient algorithms for high-dimensional and continuous
reinforcement learning
\citembullet CompLACS (EU FP7), 2011 - 2015 (PI: J.\,Shawe-Taylor)
\citembullet BOLD (ANR), 2019 - 2023  (Project Coordinator, PI: V.\,Perchet)
\citembullet BoB (ANR), 2016 - 2020  (PI: R.\,Bardenet)
\citembullet Extra-Learn (ANR), 2014 - 2018  (PI), after A.\,Lazaric
\citembullet LeLivreScolaire.fr - Sequential Learning for Educational Systems, 2017--2020 (PI)
\citembullet EduBand - (with CMU), 2015 - 2018  (coPI, with A.\,Lazaric and E.\,Brunskill)  
\citembullet  Inria/CWI, Sequential prediction \& Understanding Deep RL, postdoc funding (2016--2018)
\citembullet Allocate (with Universit\"{a}t Potsdam), 2017 - 2019  (PI) with A.\,Carpentier  
\citembullet Intel/Inria - Algorithmic Determination of IoT Edge Analytic (PI) -
2013 
\citembullet NIH grants (1R01LM010019-01A1, 1R21LM009102-01A1), 2009 - 
2013 (PI: M. Hauskrecht)
\end{category}
% -------- Publication --------------------------------------------


\begin{category}{Preprints}
\citembullet
C\^ome Fiegel, Pierre M\' enard, Tadashi Kozuno, R\' emi Munos, Vianney Perchet,  {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2212.12567}{\emph{Adapting to game trees in zero-sum imperfect information games}},
({\sf preprint}) 

\citembullet
Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo \'Avila Pires, Yash Chandak, R\'emi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, Andr\'as Gy\"orgy, Shantanu Thakoor, Will Dabney, Bilal Piot, Daniele Calandriello, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2205.14211.pdf}{\emph{Understanding self-predictive learning for reinforcement learning}},
({\sf preprint}) 

\citembullet
Tadashi Kozuno, Wenhao Yang, Nino Vieillard, Toshinori Kitamura, Yunhao Tang, Jincheng Mei, Pierre M\' enard, Mohammad Gheshlaghi Azar, {\bf Michal Valko}, R\' emi Munos, Olivier Pietquin, Matthieu Geist, Csaba Szepesv\' ari
\href{https://arxiv.org/pdf/2205.14211.pdf}{\emph{KL-entropy-regularized RL with a generative model is minimax optimal}},
({\sf preprint}) 


\citembullet
Mehdi Azabou, Mohammad Gheshlaghi Azar, Ran Liu, Chi-Heng Lin, Erik C. Johnson, Kiran Bhaskaran-Nair, Max Dabagia, 
Bernardo Avila Pires, Lindsey Kitchell,
Keith B. Hengen, William Gray-Roncal, {\bf Michal Valko}, Eva L. Dyer:
\href{https://arxiv.org/pdf/2102.10106.pdf}{\emph{Mine Your Own vieW: Self-supervised learning through across-sample prediction}},
({\sf preprint}) 
\end{category}

\begin{category}{Papers\\2022}


\citembullet
Zhaohan Daniel Guo, Shantanu Thakoor, Miruna P\^islar, Bernardo Avila Pires, Florent Altch\' e, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang,  {\bf Michal Valko}, R\' emi Munos, Mohammad Gheshlaghi Azar, Bilal Piot:   
\href{https://arxiv.org/pdf/2206.08332.pdf}
{\emph{BYOL-Explore: Exploration by bootstrapped prediction}},
Neural Information Processing Systems
({\sf NeurIPS 2022}) 

\citembullet
Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, \' Eric Moulines, R\' emi Munos, Alexey Naumov, Mark Rowland, {\bf Michal Valko}, Pierre M\' enard: 
\href{https://arxiv.org/pdf/2209.14414.pdf} 
{\emph{Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees}},
Neural Information Processing Systems
({\sf NeurIPS 2022}) 

\citembullet
Daniel Jarrett, Corentin Tallec, Florent Altch\'e, Thomas Mesnard, R\'emi Munos, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2206.08332.pdf}
{\emph{Curiosity in hindsight}},
NeurIPS 2022 Workshop: Deep reinforcement learning
({\sf NeurIPS 2022 - DeepRL}) 


\citembullet
Daniil Tiapkin, Denis Belomestny, \' Eric Moulines, Alexey Naumov, Sergey Samsonov, Yunhao Tang, {\bf Michal Valko}, Pierre M\' enard:   
\href{https://arxiv.org/pdf/2205.07704.pdf}
{\emph{From Dirichlet to Rubin: Optimistic exploration in RL without bonuses}},
International Conference on Machine Learning
({\sf ICML 2022})  {\bf [long talk: 2\%]}


\citembullet
Anirudh Goyal, Abram L Friesen, Theophane Weber, Andrea Banino, Nan Rosemary Ke, Adria Puigdomenech Badia, Ksenia Konyushkova,  {\bf Michal Valko}, Simon Osindero, Timothy P Lillicrap, Nicolas Heess, Charles Blundell:
\href{https://arxiv.org/pdf/2202.08417.pdf}
{\emph{Retrieval-augmented reinforcement learning}},
International Conference on Machine Learning
({\sf ICML 2022}) 


\citembullet
Daniele Calandriello, Luigi Carratino, Alessandro Lazaric,  {\bf Michal Valko}, Lorenzo Rosasco:
\href{https://arxiv.org/pdf/2201.12909.pdf}
{\emph{Scaling Gaussian process optimization by evaluating a few unique candidates multiple times}},
International Conference on Machine Learning
({\sf ICML 2022}) 



\citembullet
Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, R\' emi Munos,  Mehdi Azabou, Eva L. Dyer, Petar Veličković, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2102.06514.pdf}{\emph{Large-scale representation learning on graphs via bootstrapping}},
International Conference on Learning Representations
({\sf ICLR 2022}) 


\citembullet
Jean Tarbouriech, Omar Darwiche Domingues, Pierre M\' enard, Matteo Pirotta, {\bf Michal Valko}, Alessandro Lazaric: 
\href{https://arxiv.org/abs/2111.12045.pdf}{\emph{Adaptive multi-goal explorationt}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2022}) 

\citembullet
Yunhao Tang, Mark Rowland, R\' emi Munos,  {\bf Michal Valko}: 
\href{https://arxiv.org/pdf/2203.16177.pdf}{\emph{Marginalized operators for off-policy reinforcement learning}}, International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2022}) 
\end{category}


\begin{category}{Papers\\2021}





\citembullet
Ran Liu, Mehdi Azabou, Max Dabagia, Chi-Heng Lin, Mohammad Gheshlaghi Azar, Keith B. Hengen, {\bf Michal Valko}, Eva L. Dyer: Drop, Swap, and Generate:
\href{https://arxiv.org/pdf/2111.02338.pdf}{\emph{A self-supervised approach for generating neural activity}},
Neural Information Processing Systems
({\sf NeurIPS 2021}) {\bf [oral presentation]}

\citembullet
Tadashi Kozuno*, Pierre M\' enard*, R\' emi Munos, {\bf Michal Valko}: 
\href{https://arxiv.org/pdf/2106.06279.pdf}{\emph{Model-free learning for two-player zero-sum partially observable Markov games with perfect recall}},
Neural Information Processing Systems
({\sf NeurIPS 2021}) {\bf [spotlight]}


\citembullet
Jean Tarbouriech, Runlong Zhou, Simon S.\,Du, Matteo Pirotta, {\bf Michal Valko}, Alessandro Lazaric: 
\href{https://arxiv.org/pdf/2104.11186.pdf}{\emph{Stochastic shortest path: minimax, parameter-free and towards horizon-free regret}},
Neural Information Processing Systems
({\sf NeurIPS 2021})  {\bf [spotlight]}

\citembullet
Jean Tarbouriech, Matteo Pirotta, {\bf Michal Valko}, Alessandro Lazaric:
\href{http://arxiv.org/abs/2007.06437.pdf}
{\emph{A provably efficient sample collection strategy for reinforcement learning}},
Neural Information Processing Systems
({\sf NeurIPS 2021}) 



\citembullet
Yunhao Tang*, Tadashi Kozuno*, Mark Rowland, R\' emi Munos,  {\bf Michal Valko}: 
\href{https://arxiv.org/pdf/2106.13125.pdf}{\emph{Unifying gradient estimators for meta-reinforcement learning via off-policy evaluation}},
Neural Information Processing Systems
({\sf NeurIPS 2021}) 




\citembullet
Adri\`a Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patraucean, Florent Altch\' e, {\bf Michal Valko}, Jean-Bastien Grill, A\"aron van den Oord, Andrew Zisserman: 
\href{https://arxiv.org/pdf/2103.16559.pdf}{\emph{Broaden your views for self-supervised video learning}},
International Conference on Computer Vision
({\sf ICCV 2021}) 


\citembullet
Pierre M\' enard,  Omar Darwiche Domingues, Xuedong Shang,  {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2103.01312.pdf}{\emph{UCB Momentum Q-learning: Correcting the bias without forgetting}},
International Conference on Machine Learning
({\sf ICML 2021})  {\bf [long talk: 3\%]}

\citembullet
Pierre M\' enard,  Omar Darwiche Domingues, Anders Jonsson,  Emilie Kaufmann, Edouard Leurent, {\bf Michal Valko}:
\href{http://arxiv.org/abs/2007.13442.pdf}{\emph{Fast active learning for pure exploration in reinforcement learning}},
International Conference on Machine Learning
({\sf ICML 2021}) 


\citembullet
Tadashi Kozuno, Yunhao Tang, Mark Rowland, R\' emi Munos, Steven Kapturowski, Will Dabney, {\bf Michal Valko}, David Abel: 
\href{https://arxiv.org/pdf/2103.00107.pdf}{\emph{Revisiting Peng's Q($\lambda$) for for modern reinforcement learning}},
International Conference on Machine Learning
({\sf ICML 2021}) 

\citembullet
Yunhao Tang, Mark Rowland, R\' emi Munos, {\bf Michal Valko}: 
\href{https://arxiv.org/pdf/2106.06170.pdf}{\emph{Taylor expansion of discount factors}},
International Conference on Machine Learning
({\sf ICML 2021}) 

\citembullet
Xavier Fontaine, Pierre Perrault, {\bf Michal Valko}, Vianney Perchet:
\href{https://arxiv.org/pdf/1906.08509.pdf}{\emph{Online A-optimal design and active linear regression}},
International Conference on Machine Learning
({\sf ICML 2021}) 


\citembullet
Omar Darwiche Domingues, Pierre M\'enard, Emilie Kaufmann, Matteo Pirotta, {\bf Michal Valko}:
\href{https://arxiv.org/abs/2004.05599}{\emph{Regret bounds for kernel-based reinforcement learning}},
International Conference on Machine Learning
({\sf ICML 2021}) 


\citembullet
Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, William Spearman, Tim Waskett, Dafydd Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton, Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet Dutta, {\bf Michal Valko}, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Remi Munos, Trevor Back, Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore Graepel, Demis Hassabis: Game Plan: 
\href{https://arxiv.org/pdf/2011.09192.pdf}{\emph{Game plans: What AI can do for football, and what football can do for AI}},
accepted with minor revisions to Journal of Artificial Intelligence Research,
({\sf JAIR}) 


\citembullet
Omar Darwiche Domingues, Pierre M\'enard,  Matteo Pirotta, Emilie Kaufmann, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2007.05078.pdf}
{\emph{A kernel-based approach to non-stationary reinforcement learning in metric spaces}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2021}) 


\citembullet
Omar Darwiche Domingues, Pierre M\'enard,  Emilie Kaufmann, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2010.03531.pdf}
{\emph{Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited}},
Algorithmic Learning Theory
({\sf ALT 2021}) 

\citembullet
Jean Tarbouriech, Matteo Pirotta, {\bf Michal Valko}, Alessandro Lazaric:
{\emph{Sample complexity bounds for stochastic shortest path with a generative model}},
Algorithmic Learning Theory
({\sf ALT 2021}) 


\citembullet
Emilie Kaufmann, Pierre M\' enard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent, {\bf Michal Valko}:
\href{http://arxiv.org/abs/2006.06294}{\emph{Adaptive reward-free exploration}},
Algorithmic Learning Theory
({\sf ALT 2021}) 

\citembullet
Guillaume Gautier, R\' emi Bardenet, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2003.02344.pdf}
{\emph{Fast sampling from $\beta$-ensembles}},
Statistics and Computing
({\sf Statistics and Computing 2021}) 


\citembullet
Omar Darwiche Domingues, Corentin Tallec, R\' emi Munos,  {\bf Michal Valko}: 
\href{https://openreview.net/pdf?id=vRSY3L4Rlhp}{\emph{Density-based bonuses on learned representations for reward-free exploration in deep reinforcement learning}}, in 
 ICML Workshop on Unsupervised Reinforcement Learning
({\sf ICML-UnsupRL 2021}) 
\end{category}


\begin{category}{Papers\\2020}

\citembullet
Jean-Bastien Grill, Florian Strub, Florent Altch\' e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\' emi Munos, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/grill2020bootstrap}{\emph{Bootstrap Your Own Latent: A new approach to self-supervised learning}},
Neural Information Processing Systems
({\sf NeurIPS 2020}) 

\citembullet
Pierre H.\,Richemond, Jean-Bastien Grill, Florent Altch\' e, Corentin Tallec, Florian Strub,  Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/2010.10241.pdf}{\emph{BYOL works even without batch statistics}},
NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice
({\sf NeurIPS 2020 - SSL}) 


\citembullet
Jean Tarbouriech, Matteo Pirotta, {\bf Michal Valko}, Alessandro Lazaric:
{\emph{Improved sample complexity for incremental autonomous exploration in MDPs}},
Neural Information Processing Systems
({\sf NeurIPS 2020}) 

\citembullet
Daniele Calandriello*, Micha\l{} Derezi\'nski*, {\bf Michal Valko}:
\href{http://arxiv.org/abs/2006.16947}{\emph{Sampling from a k-DPP without looking at all items}},
Neural Information Processing Systems
({\sf NeurIPS 2020}) 

\citembullet
Pierre Perrault, Etienne Boursier, Vianney Perchet, {\bf Michal Valko}:
\href{http://arxiv.org/abs/2006.06613}{\emph{Statistical efficiency of Thompson sampling for combinatorial semi-bandits}},
Neural Information Processing Systems
({\sf NeurIPS 2020}) 

\citembullet
Anders Jonsson, Emilie Kaufmann, Pierre M\' enard, Omar Darwiche Domingues, Edouard Leurent,{\bf Michal Valko}:
\href{https://arxiv.org/abs/2006.05879}{\emph{Planning in Markov decision processes with gap-dependent sample complexity}},
Neural Information Processing Systems
({\sf NeurIPS 2020}) 


\citembullet
Jean-Bastien Grill, Florent Altché, Yunhao Tang, Thomas Hubert, Yunhao Tang, {\bf Michal Valko}, Ioannis Antonoglou, R\' emi Munos:
\href{https://arxiv.org/pdf/2007.12509.pdff}
{\emph{Monte-Carlo tree search as regularized policy optimization}},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
Yunhao Tang, {\bf Michal Valko}, R\' emi Munos:
\href{https://arxiv.org/pdf/1910.10945.pdf}
{\emph{Taylor expansion policy optimization}},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
R\' emy Degenne, Pierre M\' enard, Xuedong Shang, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/1910.10945.pdf}
{\emph{Gamification of pure exploration for linear bandits}},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
Pierre Perrault, Zheng Wen, Jennifer Healey, {\bf Michal Valko}:
\href{https://hal.archives-ouvertes.fr/hal-02904278/document}
{\emph{Budgeted online influence maximization}},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
Jean Tarbouriech, Evrard Garcelon, {\bf Michal Valko}, Matteo Pirotta, Alessandro Lazaric:
\href{https://arxiv.org/pdf/2003.06259.pdf}
{\emph{No-regret exploration in goal-oriented reinforcement learning}},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
Aadirupa Saha, Pierre Gaillard, {\bf Michal Valko}:
\href{https://arxiv.org/abs/2004.06248}
{\emph{Improved sleeping bandits with stochastic action sets and adversarial rewards}},
International Conference on Machine Learning
({\sf ICML 2020}) 


\citembullet
Daniele Calandriello, Luigi Carratino, Alessandro Lazaric,  {\bf Michal Valko}, Lorenzo Rosasco:
\href{https://arxiv.org/pdf/2002.09954.pdf}
{\emph{Near-linear time Gaussian process optimization with adaptive batching and resparsification}},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
Anne Manegueu, Claire Vernade, Alexandra Carpentier, {\bf Michal Valko}:
\emph{Delayed bandits with different and unknown delay distributions with unbounded support},
International Conference on Machine Learning
({\sf ICML 2020}) 

\citembullet
Jean Tarbouriech, Matteo Pirotta, {\bf Michal Valko}, Alessandro Lazaric:
{\emph{Reward-free Exploration beyond finite-horizon}},
Theoretical Foundations of RL Workshop @ ICML 2020
({\sf ICML 2020 - RL Theory}) 


\citembullet
Pierre Perrault, Vianney Perchet,   {\bf Michal Valko}:
\emph{Covariance-adapting algorithm for semi-bandits with application to sparse rewards},
Conference on Learning Theory
({\sf COLT 2020}) 

\citembullet
Xuedong Shang, Rianne de Heide,  Emilie Kaufmann, Pierre M\'enard, {\bf Michal Valko}:
\emph{Fixed-confidence guarantees for Bayesian best-arm identification},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2020}) 

\citembullet
C\^ome Fiegel, Victor Gabillon, {\bf Michal Valko}:
\emph{Adaptive multi-fidelity optimization with fast learning rates},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2020}) 

\citembullet
Victor Gabillon, Rasul Tutunov, {\bf Michal Valko}, Haitham Bou Ammar:
\emph{Derivative-free \& order-robust optimisation},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2020}) 

\citembullet
Julien Seznec, Pierre M\'enard, Alessandro Lazaric, {\bf Michal Valko}:
\emph{A single algorithm for both restless and rested rotting bandits},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2020}) 

\citembullet
Tom\'a\v s Koc\' ak,  R\' emi Munos, Branislav Kveton, Shipra Agrawal, {\bf Michal Valko}:
\emph{Spectral Bandits},
accepted for publication to Journal of Machine Learning Research
({\sf JMLR 2020}) 

\citembullet
Branislav Kveton, Zheng Wen, Azin Ashkan, {\bf Michal Valko}:
\href{https://arxiv.org/pdf/1405.7752.pdf}
{\emph{Learning to act greedily: Polymatroid semi-bandits}},
accepted for publication to Journal of Machine Learning Research
({\sf JMLR 2020}) 
\end{category}

\begin{category}{2019}
\citembullet
Jean-Bastien Grill*, Omar Darwiche Domingues*, Pierre M\'enard, R\'emi Munos, {\bf Michal Valko}: \emph{Planning in entropy-regularized Markov decision processes and games}, Neural Information Processing Systems
({\sf NeurIPS 2019})
\citembullet 
Mark Rowland, Shayegan Omidshafiei, Karl Tuyls, Julien P\'erolat, {\bf Michal Valko}, Georgios Piliouras, R\'emi Munos: \emph{Multiagent evaluation under incomplete information}, Neural Information Processing Systems
({\sf NeurIPS 2019}) 
\citembullet
Micha\l{} Derezi\'nski*, Daniele Calandriello*, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/derezinski2019exact.pdf}{\emph{Exact sampling of determinantal point processes with sublinear time preprocessing}},
Neural Information Processing Systems
({\sf NeurIPS 2019}) 
\citembullet
Guillaume Gautier, R\' emi Bardenet, {\bf Michal Valko}:
%\href{http://researchers.lille.inria.fr/~valko/hp/publications/gautier2019processus.pdf}
{\emph{On two ways to use determinantal point processes for Monte Carlo integration}},
Neural Information Processing Systems
({\sf NeurIPS 2019}) 

\citembullet
Daniele Calandriello, Luigi Carratino, Alessandro Lazaric,   {\bf Michal Valko}, Lorenzo Rosasco:
\href{http://researchers.lille.inria.fr/~valko/hp/publications/calandriello2019gaussian.pdf}{\emph{Gaussian process optimization with adaptive sketching: Scalable and no regret}},
Conference on Learning Theory
({\sf COLT 2019}) 
\citembullet
Pierre Perrault, Vianney Perchet,   {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/perrault2019exploiting.pdf}{\emph{Exploiting structure of uncertainty for efficient combinatorial semi-bandits}},
International Conference on Machine Learning
({\sf ICML 2019}) 
\citembullet
Peter Bartlett, Victor Gabillon, Jennifer Healey,    {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/bartlett2019scale-free}{
\emph{Scale-free adaptive planning for deterministic dynamics \& discounted rewards}},
International Conference on Machine Learning
({\sf ICML 2019}) 

\citembullet
Xuedong Shang, Emilie Kaufmann, {\bf Michal Valko}:
%\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/shang2019general.pdf},
{\emph{A simple dynamic bandit-based algorithm for hyper-parameter tuning}}
Workshop on Automated Machine Learning at International Conference on Machine Learning
({\sf ICML 2019 - AutoML}) 

\citembullet
Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/seznec2019rotting.pdf}{\emph{Rotting bandits are no harder than stochastic ones}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2019})   {\bf [full oral]}
\citembullet
Andrea Locatelli, Alexandra Carpentier, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/locatelli2019active.pdf}
{\emph{Active multiple matrix completion with adaptive confidence sets}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2019}) 


\citembullet
Pierre Perrault, Vianney Perchet,   {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/perrault2019finding.pdf}
{\emph{Finding the bandit in a graph: Sequential search-and-stop}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2019}) 


\citembullet
Peter Bartlett, Victor Gabillon,   {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/bartlett2019simple.pdf}
{\emph{A simple parameter-free and adaptive approach to optimization under a minimal local smoothness assumption}},
Algorithmic Learning Theory
({\sf ALT 2019}) 

\citembullet
Xuedong Shang, Emilie Kaufmann, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/shang2019general.pdf}
{\emph{General parallel optimization without metric}}
Algorithmic Learning Theory
({\sf ALT 2019}) 



\citemnobullet
Guillaume Gautier, R\' emi Bardenet, {\bf Michal Valko}:
\href{https://arxiv.org/abs/1809.07258}
{\emph{DPPy: Sampling determinantal point processes with Python}},
Journal of Machine Learning Research
({\sf JMLR 2019}) 

\citembullet
Guillaume Gautier, R\' emi Bardenet, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/publications/gautier2019processus.pdf}
{\emph{Les processus ponctuels d\'eterminantaux en apprentissage automatique}},
({\sf Gretsi 2019}) 


\end{category}

\begin{category}{2018}
\citembullet
Jean-Bastien Grill, {\bf Michal Valko}, R\' emi Munos:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/grill2018optimistic.pdf}
{\emph{Optimistic optimization of a Brownian}},
Neural Information Processing Systems
({\sf NeurIPS 2018}) 

\citembullet
Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/oyallon2018compressing.pdf}
{\emph{Compressing the input for CNNs with the first-order scattering transform}},
European Conference on Computer Vision
({\sf ECCV 2018}) 

\citembullet
Daniele Calandriello,  Ioannis Koutis,  Alessandro Lazaric, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/calandriello2018improved.pdf}
{\emph{Improved large-scale graph learning through ridge spectral sparsification}},
International Conference on Machine Learning
({\sf ICML 2018}) 

\citembullet
Yasin Abbasi-Yadkori, Peter Bartlett, Victor Gabillon, Alan Malek,  {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/abbasi-yadkori2018best.pdf}
{\emph{Best of both worlds: Stochastic \& adversarial best-arm identification}},
Conference on Learning Theory
({\sf COLT 2018}) 

\citembullet
Xuedong Shang, Emilie Kaufmann, {\bf Michal Valko}:
\emph{Adaptive black-box optimization got easier: HCT needs only local smoothness},
European Workshop on Reinforcement Learning
({\sf EWRL 2018}) 

\end{category}\begin{category}{2017}

\citembullet
Daniele Calandriello, Alessandro Lazaric, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/calandriello2017efficient.pdf}
{\emph{Efficient second-order online kernel learning with adaptive embedding}},
Neural Information Processing Systems
({\sf NeurIPS 2017}) 


\citembullet
Zheng Wen, Branislav Kveton, {\bf Michal Valko}, Sharan Vaswani:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/wen2017online.pdf}
{\emph{Online influence maximization under independent cascade model with semi-bandit feedback}},
Neural Information Processing Systems
({\sf NeurIPS 2017})


\citembullet
Guillaume Gautier, R\' emi Bardenet, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/gautier2017zonotope.pdf}
{\emph{Zonotope hit-and-run for efficient sampling from projection DPPs}},
International Conference on Machine Learning
({\sf ICML 2017}) 

\citembullet
D.\,Calandriello, A.\,Lazaric, {\bf M.\,Valko}:
\emph{Second-order kernel online convex optimization with adaptive sketching},
International Conference on Machine Learning
({\sf ICML 2017}) 

\citembullet
Daniele Calandriello, Alessandro Lazaric, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/calandriello2017distributed.pdf}
{\emph{Distributed adaptive sampling for kernel matrix approximation}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2017}) 

\citembullet
Akram Erraqabi, Alessandro Lazaric, {\bf Michal Valko}, Emma Brunskill, Yu-En Liu:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/erraqabi2017trading.pdf}
{\emph{Trading off rewards and errors in multi-armed bandits}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2017}) 

\end{category}\begin{category}{2016}

\citembullet
Jean-Bastien Grill, {\bf Michal Valko}, R\' emi Munos:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/grill2016blazing.pdf}
{\emph{Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning}},
Neural Information Processing Systems
({\sf NeurIPS 2016}) 

\citembullet
Akram Erraqabi,  {\bf Michal Valko}, Alexandra Carpentier, Odalric-Ambrym Maillard:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/erraqabi2016pliable.pdf}
{\emph{Pliable rejection sampling}},
International Conference on Machine Learning
({\sf ICML 2016}) 

\citembullet
Tom\'a\v s Koc\' ak, Gergely Neu, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/kocak2016online.pdf}
{\emph{Online learning with noisy side observations}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2016}) {\bf [full oral]}

\citembullet
Tom\'a\v s Koc\' ak, Gergely Neu, {\bf Michal Valko}:
\emph{ Online learning with Erd\H os--R\'enyi side-observation graphs},
Uncertainty in Artificial Intelligence
({\sf UAI 2016}) 

\citembullet
Daniele Calandriello, Alessandro Lazaric, {\bf Michal Valko}:
\emph{Analysis of Nystr{\"o}m method with sequential ridge leverage scores},
Uncertainty in Artificial Intelligence
({\sf UAI 2016}) 

\citembullet
Mohammad Ghavamzadeh, Yaakov Engel, {\bf Michal Valko}:
\emph{Bayesian policy gradient and actor-critic algorithms},
Journal of Machine Learning Research
({\sf JMLR 2016}) 


\citembullet
Alexandra Carpentier, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/carpentier2016revealing.pdf}
{\emph{Revealing graph bandits for maximizing local influence}},
International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2016}) 

\end{category}\begin{category}{2015}

\citembullet
Jean-Bastien Grill, {\bf Michal Valko}, R\' emi Munos:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/grill2015black-box.pdf}
{\emph{Black-box optimization of noisy functions with unknown smoothness}},
Neural Information Processing Systems
({\sf NeurIPS 2015}) 

\citembullet
Alexandra Carpentier, {\bf Michal Valko}:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/carpentier2015simple.pdf}
{\emph{Simple regret for infinitely many armed bandits}},
International Conference on Machine Learning
({\sf ICML 2015}) 


\citembullet
Manjesh Hanawal, Venkatesh Saligrama, {\bf Michal Valko},  R\' emi Munos:
\emph{Cheap Bandits},
The~32th International Conference on Machine Learning
({\sf ICML 2015}) {\bf [oral presentation]}

\citembullet
Julien Audiffren, {\bf Michal Valko},  Alessandro Lazaric, Mohammad Ghavamzadeh:
\emph{Maximum Entropy Semi-Supervised Inverse Reinforcement Learning},
 The 24th International Joint Conference on Artificial Intelligence
({\sf IJCAI 2015}) {\bf [oral presentation]}

\end{category}\begin{category}{2014}

\citembullet
Tom\'a\v s Koc\' ak, Gergely Neu, {\bf Michal Valko}, R\' emi Munos:  
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/kocak2014efficient.pdf}
{\emph{Efficient learning by implicit exploration in bandit problems with side observations}}, 
Neural Information Processing Systems ({\sf NeurIPS 2014})

\citembullet
Alexandra Carpentier, {\bf Michal Valko}: 
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/carpentier2014extreme.pdf}
{\emph{Extreme bandits}},
Neural Information Processing Systems ({\sf NeurIPS 2014})

\citembullet
Gergely Neu, {\bf Michal Valko}: 
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/calandriello2018improved.pdf}
{\emph{Online combinatorial optimization with stochastic decision sets and adversarial losses}}, 
Neural Information 
Processing Systems ({\sf NeurIPS 2014})

\citembullet
Julien Audiffren, {\bf Michal Valko},  Alessandro Lazaric, Mohammad Ghavamzadeh:
\emph{MESSI: Maximum Entropy Semi-Supervised Inverse Reinforcement Learning},
 NeurIPS Workshop on Novel Trends and Applications in Reinforcement Learning
({\sf NeurIPS 2014 - TCRL}) 


\citembullet
{\bf Michal Valko}, R\' emi Munos, Branislav Kveton, Tom\'a\v s Koc\' ak:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/valko2014spectral.pdf}
{\emph{Spectral bandits for smooth graph functions}},
International Conference on Machine Learning
({\sf ICML 2014}) 

\citembullet
Tom\'a\v s Koc\' ak, {\bf Michal Valko},  R\' emi Munos, Shipra Agrawal:
\emph{Spectral Thompson Sampling},
 The 28th AAAI Conference on Artificial Intelligence 
({\sf AAAI 2014}) {\bf [oral presentation]}

\citembullet
Philippe Preux, R\' emi Munos, {\bf Michal Valko}:
\emph{Bandits attack function optimization},
IEEE Congress on Evolutionary Computation
({\sf CEC 2014}) 


\citembullet
Tom\'a\v s Koc\' ak, {\bf Michal Valko},  R\' emi Munos, Branislav Kveton, 
Shipra Agrawal:
\emph{Spectral Bandits for Smooth Graph Functions with Applications in 
Recommender Systems},
 AAAI Workshop on Sequential Decision-Making with Big Data 
({\sf AAAI 2014 - SDMBD}) {\bf [oral presentation]}



\end{category}\begin{category}{2013}

\citembullet
{\bf Michal Valko}, Alexandra Carpentier, R\' emi Munos:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/valko2013stochastic.pdf}
{\emph{Stochastic simultaneous optimistic optimization}},
International Conference on Machine Learning
({\sf ICML 2013}) {\bf [oral presentation]}

\citembullet
{\bf Michal Valko}, Nathan Korda, R\' emi Munos, Ilias Flaounas, Nello 
Cristianini:
\emph{Finite-Time Analysis of Kernelised Contextual Bandits},
The 29nd Conference on Uncertainty in Artificial Intelligence
({\sf UAI 2013})

\citembullet
Branislav Kveton, {\bf Michal Valko}:
\emph{Learning from a Single Labeled Face and a Stream of Unlabeled Data},
The 10th IEEE International Conference on Automatic Face and Gesture Recognition
({\sf FG 2013})
{\bf [spotlight]}

\citembullet
Milos Hauskrecht, Iyad Batal, {\bf Michal Valko}, Shyam Visweswaran,
Gregory F. Cooper, Gilles Clermont: \emph{Outlier detection for patient
monitoring and alerting}, Journal of Biomedical Informatics ({\sf JBI 2013})

\end{category}\begin{category}{$\leq\ 2012$}


\citembullet
{\bf Michal Valko}, Mohammad Ghavamzadeh, Alessandro Lazaric:
\emph{Semi-supervised apprenticeship learning}, in European Workshop on
Reinforcement Learning
({\sf EWRL 2012})


\citembullet
{\bf Michal Valko}, Hamed Valizadegan, Branislav Kveton, Milos Hauskrecht:
\emph{Conditional Anomaly Detection with Soft Harmonic Functions},
International Conference on Data Mining ({\sf ICDM 2011}) 

\citembullet  
Thomas C. Hart, Patricia M. Corby, Milos Hauskrecht, Ok Hee Ryu, Richard Pelikan, {\bf Michal Valko}, Maria B. Oliveira, Gerald T. Hoehn, and Walter A. Bretz: \emph{Identification of Microbial and Proteomic Biomarkers in Early Childhood Caries}, International Journal of Dentistry ({\sf IJD 2011}) 

\citembullet  
{\bf Michal Valko}: 
\emph {Adaptive Graph-Based Algorithms for Conditional Anomaly Detection and Semi-Supervised Learning}, 
Ph.D.\,thesis, University of Pittsburgh, ({\sf PITT 2011}) 

\citembullet
{\bf Michal Valko}, Hamed Valizadegan, Branislav Kveton, Gregory F. Cooper, Milos Hauskrecht:
\emph{Conditional Anomaly Detection Using Soft Harmonic Functions: An Application to Clinical Alerting}, Workshop on Machine Learning for Global Challenges in The Twenty-Eight International Conference on
Machine Learning ({\sf ICML 2011 - Global})

\citembullet
Branislav Kveton, {\bf Michal Valko}, Ali Rahimi, Ling Huang:
\emph{Semi-Supervised Learning with Max-Margin Graph Cuts},
The 13th International Conference on Artificial Intelligence and Statistics
({\sf AISTATS 2010})

\citembullet    
{\bf Michal Valko}, Branislav Kveton, Ling Huang, Daniel Ting: 
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/valko2010online.pdf}
{\emph{Online semi-supervised learning on quantized graphs}},
Conference on Uncertainty in Artificial Intelligence
({\sf UAI 2010})

\citembullet
Milos Hauskrecht, {\bf Michal Valko},  Shyam Visweswaram, Iyad Batal, Gilles Clermont, Gregory Cooper:
\href{http://researchers.lille.inria.fr/~valko/hp/serve.php?what=publications/hauskrecht2010conditional.pdf}
{\emph{Conditional outlier detection for clinical alerting}}, 
Annual American Medical Informatics
Association conference ({\sf AMIA 2010}) 
{\bf [Homer Warner best paper award]}


\citembullet
Branislav Kveton, {\bf Michal Valko}, Matthai Phillipose, Ling Huang:
\emph{Online Semi-Supervised Perception: Real-Time Learning without Explicit
Feedback},
The Fourth IEEE Online Learning for Computer Vision Workshop in The 23rd IEEE
Conference on Computer Vision and Pattern Recognition
({\sf CVPR 2010 - OLCV})  {\bf [Google Best Paper Award]}


\citembullet
{\bf Michal Valko}, Milos Hauskrecht:
\emph{Feature importance analysis for patient management decisions}, 
13th International Congress on Medical Informatics ({\sf MEDINFO 2010})

\citembullet
{\bf Michal Valko}, Gregory Cooper, Amy Seybert, Shyam Visweswaran, Melissa Saul, Milos Hauskrecht:
\emph{Conditional anomaly detection methods for patient-management alert systems}, Workshop on
Machine Learning in Health Care Applications in The Twenty-Fifth International Conference on
Machine Learning ({\sf ICML 2008 - MLHealth})

\citembullet    
{\bf Michal Valko}, Milos Hauskrecht: \emph{Distance metric learning for conditional anomaly detection},
Twenty-First International Florida AI Research Society Conference ({\sf FLAIRS 2008})

\citembullet    
{\bf Michal Valko},  Richard Pelikan, Milos Hauskrecht: \emph{Learning predictive models for combinations of heterogeneous
proteomic data sources}, AMIA Summit on Translational Bioinformatics ({\sf
      STB 2008})  {\bf [best paper award]}

\citembullet   
Milos Hauskrecht, {\bf Michal Valko}, Branislav Kveton, Shyam Visweswaram, Gregory Cooper:
\emph{Evidence-based Anomaly Detection in Clinical Domains} in Annual American Medical Informatics
Association conference ({\sf AMIA 2007}) {\bf [nominated for the best paper award]}


\citembullet  
Wendy W. Chapman, John N. Dowling, Gregory F. Cooper, Milos Hauskrecht, {\bf Michal Valko}: \emph{A Comparison of Chief Complaints and Emergency Department Reports for Identifying Patients with Acute Lower Respiratory Syndrome} in Proceedings of the National Syndromic Surveillance Conference ({\sf ISDS 2006})

\citembullet  
Milos Hauskrecht, Richard Pelikan, {\bf Michal Valko}, James Lyons-Weiler:
\emph{Feature Selection and Dimensionality Reduction in Genomics and
Proteomics}. Fundamentals of Data Mining in Genomics and Proteomics,
eds. Berrar, Dubitzky, Granzow. Springer ({\sf 2006})

\citembullet 
{\bf Michal Valko}, Nuno C. Marques, Marco Castelani: \emph{Evolutionary Feature
Selection for Spiking Neural Network Pattern Classifiers} in
Proceedings of Portuguese Conference on Artificial Intelligence
({\sf EPIA 2005}), eds.\,Bento et al., IEEE, p.\,24--32

\citembullet 
{\bf Michal Valko} \emph{Evolving Neural Networks for Statistical Decision
Theory}, Comenius University, Bratislava, master thesis, advisor: Radoslav Harman ({\sf 2005})
\end{category}

\begin{category}{Older preprints}
\citembullet
Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Alaa Saade, Shantanu Thakoor, Bilal Piot, Bernardo Avila Pires, {\bf Michal Valko}, Thomas Mesnard, Tor Lattimore, R\' emi Munos: 
\href{https://arxiv.org/pdf/2101.02055.pdf}{\emph{Geometric entropic exploration}},
({\sf preprint}) 


\citembullet
Pierre Perrault, Jennifer Healey, Zheng Wen, Michal Valko {\bf Michal Valko}: 
\href{https://arxiv.org/pdf/2101.01631.pdf}{\emph{On the approximation relationship between optimizing ratio of submodular (RS) and difference of submodular (DS) functions}},
 ({\sf preprint}) 
\end{category}

\begin{category}{Students}
\citembullet  \emph{C\^{o}me Fiegel}, 2022 -- 2025, ENS Ulm/MVA,  Ph.D.\,student, with P.\,M\'enard and Vianney Perchet
\citembullet  \emph{Daniil Tiapkin}, 2021 -- 2024, HSE,  MSc.\,student, with A.\,Naumov, D.\,Belomestny, \'E.\,Moulines, and P.\,M\'enard 
\end{category}

\begin{category}{Postdocs}
\citembullet \textit{Pierre M\'enard}, 2019 -- 2020,  ENS Rennes/U .\,Toulouse, postdoc, Inria, with E.\,Kaufmann 
\citembullet \textit{Edouard Oyallon}, 2017 -- 2018,  ENS Rennes/ENS Ulm, postdoc, Inria 
\end{category}


\begin{category}{Past students}
\citembullet \textit{Daniel Jarrett}, 2022, University of Cambridge, visiting Ph.D.\,student, with C.\,Tallec
\citembullet \textit{Jean Tarbouriech}, 2019 -- 2022,  X/MVA, Ph.D.\,student,
Inria/FAIR, with Alessandro Lazaric
\citembullet \textit{Omar Darwiche Domingues}, 2018 -- 2022,  EC Paris/MVA, Ph.D.\,student,
Inria, with E.\,Kaufmann
\citembullet \textit{Xuedong Shang}, 2017 -- 2021,  ENS Rennes, Ph.D.\,student,
Inria, with E.\,Kaufmann
\citembullet \textit{Julien Seznec}, 2017 -- 2020, ENS Ulm/MVA, Ph.D.\,student,
Inria/Lelivrescolaire.fr, with Alessandro Lazaric and Jonathan Banon
\citembullet \textit{Pierre Perrault},  2017 -- 2020, ENS Paris-Saclay/MVA, Ph.D.\,student,
Inria/ENS Paris-Saclay, with Vianney Perchet
\citembullet \textit{Guillaume Gautier},  2017 -- 2020, EC Lille/MVA, Ph.D.\,student,
Inria/CNRS, with R.\,Bardenet
\citembullet \textit{Jean-Bastien Grill}, 2014 -- 2019, ENS Ulm/MVA, Ph.D.\,student,
Inria/ENS Paris, with R.\,Munos
\citembullet \textit{Yunhao Tang}, 2019 -- 2020, Columbia University, visiting PhD student, with R.\,Munos
\citembullet \textit{Aadirupa Saha}, 2019 -- 2020, Indian Institute of Science, Bangalore, visiting PhD student, with P.\,Gaillard
\citembullet \textit{Kaige Yang}, 2019, University College London, visiting PhD student, with P.\,M\'enard
\citembullet \textit{Rianne de Heide}, 2019, visiting PhD.\,student, CWI/Leiden University, with E.\,Kaufmann
\citembullet \textit{Daniele Calandriello}, 2014 -- 2017, PhD.\,student, Polimi, \textbf{AFIA, 1st prize}
Inria, with Alessandro Lazaric
\citembullet \textit{Tom\'a\v s Koc\' ak}, 2013 -- 2016, Comenius University, Ph.D.\,student,
Inria, with R.\,Munos

\citembullet  \emph{C\^{o}me Fiegel}, 2019, ENS Ulm, M2 student, with P.\,M\'enard and Vianney Perchet
\citembullet  \emph{Robert M\"uller}, 2020, Technical University of Munich, M2 student, with P.\,M\'enard
\citembullet  \emph{Ahmed Choukarah}, 2020, ENS Ulm, L3 student, with P.\,M\'enard
\citembullet  \emph{C\^{o}me Fiegel}, 2019, ENS Ulm, L3 student, with V.\,Gabillon
\citembullet \textit{Axel Elaldi}, 2018, master student, 
\'Ecole Centrale de Lille
\citembullet \textit{Xuedong Shang}, 2017, master student, 
ENS Rennes, with E.\,Kaufmann
\citembullet \textit{Guillaume Gautier}, 2016, master student, 
ENS Paris-Saclay, with R.\,Bardenet
\citembullet \textit{Andrea Locatelli}, 2015 -- 2016, master student,
ENS Paris-Saclay, with A.\,Carpentier 
\citembullet \textit{Akram Erraqabi}, 2015, master student,
\'Ecole Polytechnique, Paris 
\citembullet \textit{Souhail Toumdi}, 2015 -- 2016, master student, \'Ecole
Centrale de Lille,  with R. Bardenet
\citembullet \textit{Mastane Achab}, 2015, master student,
\'Ecole Polytechnique, Paris, with G.\,Neu
\citembullet \textit{Jean-Bastien Grill}, 2014, master student,
ENS Paris, with R. Munos
\citembullet \textit{Alexandre Dubus}, 2012 -- 2013, master student, Universit\'e
Lille1 -
Sciences et Technologies
\citembullet \textit{Karim Jedda},  2012--2013, master student, \'Ecole
Centrale de Lille
  \citembullet \textit{Alexis Wehrli}, 2012--2013 master student, \'Ecole
Centrale de Lille
%\citembullet master students
%\begin{itemize}
%\item Alexandre Dubus, master student, Universit\'e Lille1 - Sciences et
%Technologies
%\item Karim Jedda, master student, \'Ecole Centrale de Lille
%\item Alexis Wehrli, master student, \'Ecole Centrale de
%Lille
%\end{itemize}
\end{category}

\begin{category}{Invited Talks}
	\citembullet
	\emph{Learning by bootstrapping: Representation and Reinforcement Learning} (two master classes), 
         January 26-27, 2023, Marrakesh, Morocco
	 ({\sf Tech'Innov 2023})
	\citembullet
	\emph{Learning by bootstrapping}, 
	 Vedatour, Bratislava, Slovakia
	 ({\sf ESET 2022})
	\citembullet
	\emph{Learning by bootstrapping} (keynote), 
	 Runtime Verification, September 28-30, 3022, Tbilisi, Georgia
	 ({\sf RV 2022})
\citembullet
\emph{Best of both worlds for best-arm identification}, 
 Simons program on Data-driven decision processes, September 12-16, 2022, Simons institute, Berkeley, California, USA
 ({\sf Berkeley 2022})
\citembullet
\emph{Model-free learning for two-player zero-sum partially observable Markov games with perfect recall}, 
 ELLIS unit Milan workshop, September 8-9, 2022,  Milan, Italy  
 ({\sf Milan 2022})
\citembullet
\emph{Reinforcement learning (lecture) and BYOL-Explore (talk)}, 
 Eastern European Machine Learning Summer School, July, 6-14, 2022, Vilnius Lithuania
 ({\sf EEML 2022})
\citembullet
\emph{Bootstrap your own latent}, 
Presented on September 11th, 2021 at  DataFest Yerevan
({\sf DataFest 2021})
\citembullet
\emph{Bootstrapped representation learning on graphs,} 
Presented on June, 23rd 2021 at  Science Academy of Turkey Machine Learning Summer School
({\sf BAY\"OYO  2021})
\citembullet
\emph{Graphs in Machine Learning,} 
Invited guest lecture at Medical University Graz, Austria, June 2021
({\sf HCAI  2021})
\citembullet
\emph{Bootstrap Your Own Latent: A new approach to self-supervised learning} 
Presented in January  2021 at MIST conference in Rajeck\' a Lesn\' a 
({\sf MIST  2021})
\citembullet
\emph{Bootstrap Your Own Latent: A new approach to self-supervised learning}, 
 Presented in December  2020 at Reinforcement Learning seminar at Polish Academy of Sciences
({\sf PAS  2020})
\citembullet
\emph{Bootstrap Your Own Latent: A new approach to self-supervised learning}, 
Presented in December 16th, 2020 at Google,
({\sf GOOG  2020})
\citembullet
\emph{BYOL works even without batch statistics} 
Presented in December  2020 at Deep learning seminar at UPJS, Ko\v{s}ice, Slovakia
({\sf UPJS  2020})
\citembullet
\emph{Reinforcement learning} (minicourse),
Presented during Math of Machine Learning Winter Schoolduring in February 19-18, 2020 in Sochi, Russia 
({\sf Sochi  2020})
\citembullet
\emph{Graphs are the new gold: The power of graphs in speeding up online learning and decision making},
Presented during October 16-18, 2019 in FAST, Yerevan, Armenia 
({\sf GIF  2019})
\citembullet
\emph{Gaussian process optimization with adaptive sketching: Scalable and no regret},
Presented during September 26-2è, 2019 at Recent developments in kernel methods, UCL, London, UK
({\sf LanDeep 2019})
\citembullet
\emph{Rotting bandits are not harder than stochastic ones},
Presented during September 25-26, 2019 in Lancaster and Deepmind Bandit Workshop, Imperial, London, UK
({\sf LanDeep 2019})
\citembullet
\emph{Graphs are the new gold: The power of graphs in speeding up online learning and decision making},
Presented on July 23th, 2019, for Cisco in Krak\'ow, Poland
({\sf Cisco  2019})
\citembullet
\emph{How the negative dependence broke quadratic barrier for learning with graphs and kernels},
Presented July 5th, 2019 at Yandex HQ,Moscow, Russia
({\sf Yandex  2019})
\citembullet
\emph{Invited talk on research achievements in machine learning},
Presented during July 3-8th, at~RAAI Summer School, Moscow Institute of Physics and Technology, Russia
({\sf RAAI  2019})
\citembullet
\emph{How the negative dependence broke quadratic barrier for learning with graphs and kernels},
Presented during June 14-15th, at ICML workshop on negative dependence, Long Beach, California, USA 
({\sf ICML  2019})
\citembullet
\emph{Active block-matrix completion with adaptive confidence sets}, Presented on May 28th, 2019 at Comenius University in Bratislava, Slovakia 
({\sf CU 2016})
\citembullet
\emph{10-year road to breaking the quadratic barrier for graphs and matrices}, Presented on February 22nd, 2019 at Comenius University in Bratislava, Slovakia 
({\sf CU 2016})
\citembullet
\emph{Graphs are the new gold}, Presented on February 20th, 2019 at P.J.\,\v{S}af\'arik University in Ko\v{s}ice, Slovakia
({\sf UPJS 2016})
\citembullet
\emph{The power of graphs in speeding up online learning and decision making},
Presented on January 25th, Department of Pure Mathematics and Mathematical Statistics, University of Cambridge, UK
({\sf Cambridge  2019})
\citembullet
\emph{The power of graphs in speeding up online learning and decision making},
Presented on January 8th, Verimag, CNRS Grenoble, France
({\sf CNRS  2019})
\citembullet
\emph{A simple parameter-free and adaptive approach to optimization under a minimal local smoothness assumption},
Presented on January 7th, Verimag, CNRS Grenoble, France
({\sf CNRS  2019})
\citembullet
\emph{The power of graphs in speeding up online learning and decision making},
Presented on October 23rd, DeepMind, London, UK
({\sf DeepMind  2018})
\citembullet
\emph{Active block-matrix completion with adaptive confidence sets},  Presented on September 10--13th, 2018,  International Workshop  on Optimization and Machine Learning, CIMI, Toulouse ({\sf CIMI 2018})
\citembullet
\emph{Online influence maximization},  Presented on May 14th, 2018, Workshop on Graph Learning, LINCS, Paris ({\sf LINCS 2018})
\citembullet
\emph{Recommender systems},  Presented on March 22nd, 2018, Journ\' ee Big data, Polytech'Lille ({\sf Polytech'Lille 2018})
\citembullet
\emph{Pliable rejection sampling}, Presented on February 8th, 2018 at GDR Isis, T\' el\' ecom ParisTech in Paris ({\sf ISIS 2018})
\citembullet
\emph{Graph Bandits}, Presented on January 7th, 2018 at MIST conference in Rajeck\' a Lesn\' a ({\sf MIST 2018})
\citembullet
\emph{SequeL, graphs in ML, and online recommender systems}, Presented on November 9th, 2017 at Plateau Inria Euratechnologies in Lille, France  
({\sf Euratechnologies 2017})
\citembullet
\emph{Sequential sampling for kernel matrix approximation and online learning}
Presented on September 19th, DeepMind, London, UK
({\sf DeepMind  2017})
\citembullet
\emph{Active learning on networks and online influence maximization}, Presented on  September 18th, 2017, Decision Theory and Network Science: Methods and Applications, Lancaster, UK 
({\sf STOR-i  2017})
\citembullet
\emph{Side observation in graph bandits}, Presented on July 11th, 2017, ICML 2017 workshop on Picky Learners, Sydney, Australia ({\sf ICML 2017})
\citembullet
\emph{Distributed sequential sampling for kernel matrix approximation}, Presented on June 28th, 2017, L'Institut de Math\' ematiques de Toulouse, France
({\sf IMT 2017})
\citembullet
\emph{Online sequential solutions for recommender systems}, Presented on June 14th, 2017 at Journ\'ees Scientifiques Inria 2017 in Nice, France  
({\sf JS 2017})
\citembullet
\emph{Comment maximiser la d\'etection des influenceurs sur les r\'eseaux sociaux ?}, popularization talk, Presented on May 30th, 2017  at 13 France 
({\sf Inria 13:45 2017})
\citembullet
\emph{Where is Justin Bieber?}, Presented on March 30th, 2017  at Dating day  in Lille, France 
({\sf Dating 2017})
\citembullet
\emph{Distributed sequential sampling for kernel matrix approximation}, Presented on  March 22nd, 2017, for Universit\" at Potsdam at Amazon 
({\sf Berlin 2017})
\citembullet
\emph{Graphs in online machine learning}, Presented on December 21st, 2016 at Textkernel talk series in Amsterdam, Netherlands 
({\sf TK 2016})
\citembullet
\emph{Where is Justin Bieber?}, Presented on September 22nd, 2016 at Comenius University in Bratislava, Slovakia 
({\sf CU 2016})
\citembullet
\emph{Bandit learning}, Presented on September 15--19th, 2016 at Information technologies - Applications and Theory, at Tatransk\' e Matliare, High Tatras, Slovakia
({\sf ITAT 2016})
\citembullet
\emph{Decision-making on graphs without graphs}, Presented on June 16-17th, 2016 at Graph-based Learning and Graph Mining workshop, at Inria Lille, France
({\sf GBLGM 2016})
\citembullet
\emph{Sequential learning on graphs with limited feedback}, Presented on May 11--13th, 2016 at Data Driven Approach to Networks and Language, at ENS Lyon, France
({\sf NETSpringLyon 2016})
\citembullet
\emph{Benefits of Graphs in Bandit Settings}, Presented on January 11--12th, 2016 at Multi-armed Bandit Workshop 2016 at STOR-i, Lancaster University, UK 
({\sf STOR-i 2016})
\citembullet
\emph{Online decision-making on graphs: Smoothness and Side Observations},
 Presented  at DaSciM, LIX, \'Ecole Polytechnique, France, April 14th, 2015 
({\sf X 2015})
\citembullet
\emph{Bandits on Graphs: Exploiting Smoothness and Side Observations},
 Presented  at CMLA, ENS Paris-Saclay, France, December 16th, 2014 
({\sf ENS 2014})
\citembullet
  \emph{Optimistic Optimization},
 Presented  at MIST conference, Fa\v ckovsk\' e sedlo, Slovakia, January 7th, 2014
({\sf MIST 2014})
\citembullet
\emph{Sequential Face Recognition with Minimal Feedback},
Presented at 30 minutes of Science, Lille, May 2nd, 2013 ({\sf Inria 2013})
\citembullet
 \emph{One Class Learning From Streams of Unlabeled Data},
Presented at Large-scale Online Learning and Decision Making Workshop,
April 28th, 2012 ({\sf LSOLDM 2012})
\citembullet
 \emph{Scaling Graph-Based Algorithms}, Presented at
LAMPADA workshop, July 20th, 2012 ({\sf LAMPADA 2012})
\citembullet
\emph{Large Scale Sequential Learning}, opening speaker at
Slovak Oxford Science, April 28th, 2012 ({\sf Oxford UK 2012})
\citembullet
 \emph{Adaptive Graph-Based Algorithms}, Presented on July
6th, 2011 at Microsoft Research Redmond ({\sf MSR Redmont 2011})
\citembullet
 \emph{Online Semi-Supervised Learning}, Presented in 2011
at MPI T\"{u}bingen, Germany ({\sf MPI Tuebingen 2011})
\citembullet
 \emph{Semi-supervised Learning with Random Walks on
Graphs}, Presented at 6th Comenius University Alumni conference ({\sf TAM 2009})
\end{category}

\begin{category}{Demos, Presen-- tations}
\citembullet
{\bf Michal Valko}:  \emph{Graph-Based Anomaly Detection with Soft Harmonic
Functions}, Presented at
CS Department Research Competition  ({\sf 2011}), also at CS Day ({\sf 2011})
and  Grad Expo ({\sf 2011}) {\bf [\first place]}

\citembullet
Branislav Kveton, {\bf  Michal Valko}, Matthai Philiposse:  \emph{Real-Time
Adaptive Face Recognition}, Presented at
23rd Neural Information Processing Systems conference ({\sf NeurIPS 2009}),
Demonstration

\citembullet
{\bf  Michal Valko}, Branislav Kveton, Matthai Philiposse:  \emph{Robust Face
Recognition Using Online Learning}, Presented at
9th University of Pittsburgh Science conference ({\sf SCIENCE 2009}) Live Demo
({\sf CS Day 2010}) Poster ({\sf Grad Expo 2010}) Talk

\citembullet
{\bf Michal Valko}:  \emph{Conditional anomaly detection with adaptive
similarity metric}, Presented at
CS Department Research Competition  ({\sf 2008 }) {\bf [\first place]}

\citembullet
{\bf Michal Valko}, Milos Hauskrecht, G. Cooper, S. Visweswaran, M. Saul, A.
Seybert,  J. Harrison, A. Post:
\emph{Conditional Anomaly Detection}, Presented at ({\sf CS Day 2008}), Poster
 {\bf[\first  place by people's choice, \second by faculty]} also at ({\sf Grad
Expo 2008})
 \end{category}

%\newpage

\begin{category}{Media}
\citembullet
We have a chance to improve humanity's access to energy and medicine, says an artificial intelligence expert - Interview with Zuzana Vitkov\' a, Journal ``Denn\'ik N", Slovakia  ({\sf October 2022})
\citembullet
Scientist Valko on AI and curiosity - Interview with Bra\v no Dob\v sinsk\' y - Journal ``Aktuality.sk", Slovakia ({\sf October 2022})
\citembullet
Science in the world - Interview with D\' a\v sa Omastov\' a  - Radio Slovakia ({\sf October 2022})
\citembullet
Interview in the TV show ``Science and technology", Slovakia ({\sf October 2022})
\citembullet
Interview - Morning TV News, RTVS, Slovakia ({\sf October 2022})
\citembullet
Vedec Valko: Empatick\' a umel\' a inteligencia je sci-fi - Journal Sme, Slovakia ({\sf December 2021})
\citembullet
O umelej inteligencii a strojovom učen\' i s Michalom Valkom: .t\' y\v zde\v n .jednoducho veda ({\sf October 2021})
\citembullet
Slovenský vedec Michal Valko \v zije vo Francúzsku, kde sa \v specializuje na v\' yskum umelej inteligencie - R\' adio Expres ({\sf October 2021})
\citembullet
Veda podľa Valka najviac prosperuje, ke\v d si vedci maj\' u  \v cas kl\' as\v t ot\' azky - Rozhovory ZKH ({\sf October 2021})
\citembullet
L’apprentissage automatique apprivoise les graphes - Data Analytics Post ({\sf July 2021})
\citembullet
BYOL and self-supervised learning - Data Analytics Post ({\sf April 2021})
\citembullet
Discussion Science helps - Covid-19 ({\sf December 2020})
\citembullet
Slovak national TV: Zaostren\' e - Správy RTVS - profil  ({\sf November 2020})
\citembullet
Slovak journal: Pravda ({\sf November 2020})
\citembullet
Slovak journal: Hospod\'arske noviny - AI ({\sf  October 2020})
\citembullet
Slovak national radio: RTVS R\'adio DEV\'IN - Akad\'emia ({\sf  October 2020})
\citembullet
Radio Slovakia International - Slovakia Today with Jonathan McCormick ({\sf  October 2020})
\citembullet
ESET Science Awards ({\sf  October 2020}) 
\citembullet
Forbes - Nahradia roboti lek\'arov a bud\'u obchody bez pokladn\'i?({\sf  October 2020})
\citembullet
Slovak national radio: Rádio FM - Hľadáme algoritmy na pomoc klíme či opravu tkaniva, zaujímajú nás veľké problémy - video ({\sf  October 2020})
\citembullet
SME podcast Klik - Klik špeciál: Aj umel\'a inteligencia zvykne katastroficky zab\'uda\v{t} ({\sf  October 2020})
\citembullet
Forbes Technologies - Ako algoritmy a AI menia n\'a\v{s}  ka\v{z}dodenný \v{z}ivot ({\sf  October 2020})
\citembullet
Wall Street Journal ({\sf  August 2020}) --- Self-supervised learning
\citembullet
Denn\' ik N - Slovak national newspaper ({\sf  July 2020}) --- We want to solve problems that would deserve a Nobel Prize
\citembullet
\#NEWSam - Armenian national TV ({\sf  November 2019}) --- AI will be much more communicative in next 5-10 years
\citembullet
Invited to speak at ARTE on AI ({\sf October 2019})
\citembullet
\emph{Invited to speak at France Culture --- La M\'ethode scientifique} ({\sf April 2019})
\citembullet
\emph{Daniele Calandriello wins the prize for the Best AI Thesis in France in 2018} 
at outlets of Inria, CNRS, Lille1,  Actu,  La Voix du Nord, Newstank
({\sf April 2018})
\citembullet
\emph{Adobe research highlights our work on online influence maximization presented at NeurIPS 2017} ({\sf January 2018})
\citembullet
\emph{CNRS publishes a French article about zonotope sampling presented at ICML} ({\sf 2017})
\citembullet
\emph{Julien Seznec, our PhD student publishes an article in  Les Echos that discusses ML for education} ({\sf November 2017})
\citembullet
\emph{Interview A.\,Lazaric about our work on ML for education}, at inria.fr ({\sf December 2016})
\citembullet
\emph{Interview with N.\,Vayatis and M.\,Valko Graphs in ML course at ENS/MVA} ({\sf July 2015})
\citembullet
\emph{Interview with Rue89 about machine learning at Inria} ({\sf June 2015})
\citembullet
\emph{Intel advertising face recognition} (February 2015) ({\sf February 2015})
\citembullet
\emph{Biometric applications will soon be part of our daily life} at ARTE Future ({\sf November 2014})
\citembullet
\emph{Face Recognition} at Sciences et Avenir ({\sf July 2014})
\citembullet
\emph{Ford and Intel Mobii project using Face Recognition}, at engadget.com ({\sf June 2014}) 
\citembullet
\emph{Ford prototype using Face Recognition} at intel.com ({\sf June 2014}) 
\citembullet
\emph{Intel collaborates with Inria on Face Recognition}, at inria.fr ({\sf March 2013}) 
\citembullet
\emph{Studying abroad} at Bussiness Magazine Profit/Trend  ({\sf 2010})
 \end{category}






%\newpage

% -------- Work experience --------------------------------------------

\begin{category}{Prior Work \\experience}
% \citem{Inria - team SequeL}, Lille, France (2011 -- now)\\
% Bandits. Inverse reinforcement learning. Semi-Supervised learning.

\citem{Intel Labs}, Intel, Santa Clara, CA (2010) \\
Multi-manifold learning. Large scale semi-supervised learning.
\citem{Intel Research}, Intel, Santa Clara, CA (2009)\\
Online semi-supervised learning. Max-margin structured prediction.

\citem{Research Assistant}, University of Pittsburgh (2007 -- 2011)\\
Conditional Anomaly Detection project: System for Anomaly Detection in Medicine

\citem{Research Assistant}, University of Pittsburgh (2006)\\
Bioinformatics: Tools for preprocessing, analysis of high-throughput proteomic and genomic data and biomarker discovery.

\citem{Teaching Assistant}, University of Pittsburgh (Fall 2005)\\ 
CS7 course: Introduction to Programming 

\citem{Research Assistant}, Institute of Normal and Pathological Physiology (2003 -- 2005)\\
Slovak Academy of Sciences, Bratislava, Slovakia
%\begin{itemize} 
%\item 
%\end{itemize}

\citem{Research Fellow}, Centro de Intelig\^encia Artificial, (Spring 2005)\\
Universidade Nova de Lisboa, Portugal

\citem{Organizer and Lecturer}, Math Seminars in Slovakia (1998 -- 2005)\\ 
Math Competitions, Math Summer Camps, Slovakia

%\citem{Cashier/Cook/Pizza maker}, Work and Travel USA (Summer 2004) \\
%Sandwich Haven \& Pizza, Marthas Vineyard, MA, USA
%
%\citem{Web developer/designer}, freelance (1996 -- 2004)\\ 
%
\end{category}

% -------- Professional Activities --------------------------------------------

\begin{category}{Service Activities}
\citembullet Organizing co-chair: TS Workshop: Optimizing Human Learning (ITS 2018, 2019)
\citembullet Organizing co-chair: The power of graphs in machine learning and sequential decision-making workshop (GPOWER 2019)
\citembullet Organizing committee: JFPDA (2013), EWRL (2018), RLSS (2019)
\citembullet Program Co-chair: CNRS summer school (RESCOM 2018)
\citembullet Research grant reviewer: FNRS (2014--now), ISF (2019--now)
\citembullet Area chair and senior program committee:  NeurIPS (2018--2020), ICLR  (2021)
\citembullet Senior program committee:  IJCAI (2017)
\citembullet Program committee: COLT (2019), ICML (2018), AISTATS (2016--2017, 2019), AAAI (2012, 2015), IJCAI (2015), RLDM (2015), EWRL 
(2012, 2015--2016), JFPDA (2014), GRL+ (2020), GNNSys (2021)
\citembullet  IEEE TPS (2018), IEEE TPAMI (2017), JMLR (2016,2018,2020), Automatica (2016--2018), NeurIPS (2012--2017), ICLR (2019), ICML (2012--2016, 2019), COLT
(2014, 2017--2018), ALT (2019), UAI (2011--2012), IJCAI (2009), KDD (2011), AAAI (2009, 2014), ECML
(2012), MEDINFO (2010)
\citembullet DeepMind mentorship program
\citembullet INTEL/Inria - Algorithmic Determination of IoT Edge Analytic -
2013 (project leader)
\citembullet  European FP7 grant (CompLACS), ANR grant (ExtraLearn), NIH grants
\citembullet Erasmus agreement between EC Lille and CU Bratislava in Computer
Science.
\citembullet Committee of experts for hiring junior faculty at CMLA, ENS Paris-Saclay  (2017)
\citembullet National Inria acceptance committee for hiring junior researchers (2017)
\citembullet Elected member of Inria Evaluation Committee (CE Inria 2014 -- 2015, 2015 -- 2019)
\citembullet Hiring committee for junior researchers at Inria Nancy (2015)
\citembullet Hiring committee for junior researchers at Inria Sophia Antipolis (2016)
\citembullet Hiring committee for junior researchers at Inria Saclay (2017)
\citembullet National committee for the secondments at Inria (2018)
\citembullet Selection committee for Inria award for scientific excellence - juniors (2015 -- 2017)
\citembullet Selection committee for Inria award for scientific excellence - confirmed (2016 -- 2018)
\citembullet Inria work group for avoiding conflicts of interest (2015 -- 2019)
\citembullet Inria work group for the creation of team RandOpt (2017-2018)
\citembullet Promotion committee for junior researchers at Inria (2014, 2015, 2018)
\citembullet Member of Slovak Mathematicians and Physicists Scientific Society 
(2000 -- present)
\citembullet Member of Slovak Chemical Society (1997 -- 2002)
\end{category}

% -------- Academic Competitions --------------------------------------------
\begin{category}{Thesis committees}
\citembullet \emph{R\'emi J\'ez\'equel}, Inria / \'Ecole Normale Sup\'erieure, April, March 2023, \emph{Examiner}
\citembullet \emph{Yoan Russac}, \'Ecole Normale Sup\'erieure, Paris, March 2022, \emph{Examiner}
\citembullet \emph{Nicolas Vecoven}, The University of Li\`ege, Belgium, January 2022, \emph{Examiner}
\citembullet \emph{Guillaume Salha}, \'Ecole Polytechnique, Paris, January 2022,  \emph{Examiner}
\citembullet \emph{Mastane Achab}, T\' el\'ecom ParisTech,  July 2019, \emph{Reviewer}
\citembullet \emph{Stratis Limnios},  \'Ecole Polytechnique, Paris, July 2019, \emph{Examiner}
\citembullet \emph{Aristide Tossou}, Chalmers University of Technology, December 2019, 
Sur la notion d'optimalit\'e dans les probl\`emes de bandits stochastiques. \emph{Reviewer and Opponent}
\citembullet \emph{Tommaso Renato Cesari}, Universit\`a degli studi di Milano, November 2019 --- 
Algorithms, Learning, and Optimization. \emph{Reviewer}
\citembullet \emph{Zhenyu Liao}, CentraleSup\'elec, Gif-sur-Yvette ---Theorie des matrices al\'eatoires pour l’apprentissage automatique en grande
dimension et les reseaux de neurones, September 2019. \emph{Examiner}
\citembullet \emph{Pierre M\'enard}, Universit\'e Toulouse 3 Paul Sabatier, June 2018 ---
Sur la notion d'optimalit\'e dans les probl\`emes de bandits stochastiques. \emph{Reviewer}
\citembullet \emph{Cl\'ement Bouttier}, Universit\'e Toulouse 3 Paul Sabatier, June 2017 ---
Optimisation globale sous incertitude: algorithmes stochastiques et
bandits continus avec application aux performances avion. 
\emph{Reviewer}
\citembullet \emph{Thibault Li\' etard}, Universit\'e Lille, September 2018 ---
Adaptive graph learning with application
to natural language processing,
\emph{Ph.D.\,mid-term evaluation reviewer}
\citembullet \emph{Mariana Vargas Vieyra}, Universit\'e Lille, September 2017 ---
Adaptive graph learning with application to natural language processing,
\emph{Ph.D.\,mid-term evaluation reviewer}
\citembullet \emph{Beno\^{ı}t Choffin}, LRI/CentraleSupelec - Universit\'e Paris-Saclay, July 2019  ---
Planning pedagogical activities on an e-learning platform for
sustainable learning,
\emph{Ph.D.\,mid-term evaluation reviewer}
\citembullet \emph{Guillaume Salha}, \'Ecole Polytechnique, Paris, November 2020  ---
Contributions to Representation Learning on Graphs with Autoencoders,
\emph{Ph.D.\,mid-term evaluation reviewer}
\citembullet \emph{Georgios Dasoulas}, \'Ecole Polytechnique, Paris, November 2020 ---
The expressive power of graph neural networks and its impact on structured representation learning
\emph{Ph.D.\,mid-term evaluation reviewer}
\end{category}

\begin{category}{Master thesis committees}
\citembullet \emph{MVA M2 master}, 10 students, 2019, \emph{Tutor and Jury Member}
\citembullet \emph{MVA M2 master}, 8 students, 2018, \emph{Tutor and Jury Member}
\citembullet \emph{MVA M2 master}, 6 students, 2017, \emph{Tutor and Jury Member}
\end{category}


% -------- Academic Competitions --------------------------------------------

\begin{category}{Contests} 
%\citembullet Best Graduate Research, Computer Science, University of Pittsburgh, 2011
%\citembullet Best Graduate Research, Computer Science, University of Pittsburgh, 2008
\citembullet \first place, Slovak Mathematical Olympiad, regional final 1993, 1994, 1996
\citembullet \nineth place, Programming Contest Zenit (national final) 1998
\citembullet  Correspondence seminars in Computer Science and Math  1992 -- 2000
consistently ranked in top 10 nation--wide

\end{category}

% ------- Skills ------------------------------------------------------

\begin{category}{Skills \& Hobbies}
% \citembullet C/C$^{++}$, Java, LISP, Maple, R, Matlab, SmallTalk, PHP, 
%   SQL, \TeX, GTK$^+$, Delphi, CL
%\citembullet perl, csh, Tcl/Tk, awk
\citembullet Certificate in Academic Entrepreneurship
\citembullet English, French, Czech and Slovak (native language), 
\citembullet Academic Senate Member, Comenius University, Bratislava, Slovakia (2003 -- 2005)
\citembullet Volunteer, Tree of Life, environmental group (2003)
\citembullet Volunteer, Comptoir de Cana (2018 -- now)
\citembullet Volunteer, Association la Cl\' e (2014 -- 2017)
\citembullet Volunteer, PASS Senior (2013 -- 2017)
\citembullet Sports: hiking, squash, racquetball, running, volleyball, swimming
\citembullet Organizer of various correspondence math seminars (KMS, STROM, SKMS) (1998 -- 2005)
\citembullet Volleyball Player, TU Slavia, Kosice (1998 -- 2000)
\citembullet Choir Singer - Tenor 2a: C\oe{}li et terra (ch\oe{}ur de chambre, 2012 -- now)
Madrigal de Lille (2011 -- 2014), Ch\oe{}ur R\'egional Nord-Pas-de-Calais Madrigal de Lille (2011 -- 2013)
University of Pittsburgh Men's Glee Club (2009 -- 2011), First Baptist Choir (2007 -- 2009), St.\,Paul's Choir (2007), Dominik Choir (1990--1991). %
%\citemnobullet \quad 
Taken private lessons with No\' emi Capron (2013 -- now), Maurice Bourbon (2012--2015),  Richard Earl Teaster (2007 -- 2011), and Claudia Pinza (2007)
\end{category}

\begin{category}{Research Projects}

\citembullet \emph{\bf Graph Bandits} with Gergely Neu, Tom\'a\v s Koc\' ak, R\'emi Munos, Shipra Agrawal, Alexandra Carpentier, Branislav Kveton, Zheng Wen
(2013 - present)\\
 Bandit problems are online decision-making problems where the only feedback given to the learner is a (noisy) reward of the chosen decision. In early sequential decision-making research, we treated each of the decisions independently. While this is enough when the number of actions is very small, it becomes impractical (both theoretically and in practice) when the set of potential actions comprises larger sets, such as a set of movies or products in a recommender system. The minimax regret guarantees scale as $\Theta(\sqrt{NT})$  where $N$ is the number of actions and $T$ is the time horizon. If $N$ happens to be large (such as the number of movies that is in millions), these guarantees are weak. Luckily, the problems become easier if there is efficient information sharing between the actions. For graphs structure, We study the benefits of homophily (similar actions give similar rewards) under the name spectral bandits, side information (well-informed bandits), and influence maximization (IM bandits). In the algorithms, we take advantage of these similarities in order to (provably) learn faster. With respect to the guarantees my colleagues and I derived, we replaced $N$ (number of actions = number of nodes in a graph) with some graph-dependent quantity, possibly smaller than $N$ if the graph structure is helpful.

\citembullet \emph{\bf SQUEAK: Online sparsification of kernels and graphs} with Daniele Calandriello, Alessandro Lazaric, Yiannis Koutis (2009 - 2018)\\
 My PhD thesis ended with an open direction, whether efficient spectral sparsifiers can fuel online graph-learning methods to make online learning with similarities even possible, i.e., with guaranteed performance and non-increasing time-step complexity. In the offline case, this was done already by Spielman et. al (2004). The difficulty of the online case is that we need to deal with the relevance of the data that we have not seen yet.  For the problem of spectral approximation in a RKHS, we introduce the first dictionary-learning streaming algorithm that operates in a single-pass over the dataset. Previous results (Alaoui and Mahoney, 2015; and Bach 2013) had either a quadratic-time complexity, or a space complexity that scaled with the coherence of the dataset, a quantity always larger than the effective dimension. Prior methods have also two major drawbacks: (1) they require multiple passes over the data or alternatively random access to the dataset, and (2) they have inherent bottlenecks that make it difficult to parallelize them. We introduce a new single-pass streaming RLS sampling approach that sequentially constructs the dictionary, where each step compares a new sample only with the current intermediate dictionary and not all past samples. We prove that the size of all intermediate dictionaries scales only with the effective dimension of the dataset, and therefore guarantee a per-step time and space complexity independent from the number of samples. This reduces the overall time required to construct provably accurate dictionaries from quadratic to near-linear, or even logarithmic when parallelized. Finally, for many non-parametric learning problems (e.g., K-PCA, graph SSL, online kernel learning) we show that we can use the generated dictionaries to compute approximate solutions in near-linear that are both provably accurate and empirically competitive.


\citembullet \emph{\bf Sample efficient Monte-Carlo tree search: TrailBlazer, SmoothCruiser, StoSOO, POO, and OOB} with Jean-Bastien Grill, R\'emi Munos, 
Alexandra Carpentier (2011 - now)\\
Monte-Carlo planning and Monte-Carlo tree search has been popularized in the game of computer Go (Coulom 2007, Gelly 2006, Silver 2016) and shown impressive performance in many other high dimensional control and game problems (Browne 2012). The empirical success of UCT on one side but the absence of performance guarantees for it on the other, incited research on similar but theoretically founded algorithms. Our first contribution are generic \emph{black-box function optimizers} for extremely difficult functions (extremely nonsmooth, no derivatives?) with guarantees with main application to hyper-parameter tuning. The second set of contributions in \emph{planning}. The first example is \emph{TrailBlazer}, adaptive planning algorithm in MDPs (Markov decision process).


\citembullet \emph{\bf Adaptive structural sampling} with Alexandra Carpentier, Andrea Locatelli, Akram Erraqabi,Alessandro Lazaric, R\' emi Bardenet,Guillaume Gautier
 (2013 - 2020)\\
Many of the sequential problems require adaptive sampling in some particular way.  One example is using learning to improve rejection rate in rejection sampling by learning the proposal. [Erraqabi et al. ICML 2015]. Another one is sampling with two contradictory objectives such as when we have to trade off reward and regret [Erraqabi et al. AISTATS 2016]. Other examples include extreme [Carpentier and Valko, NeurIPS 2014] and infinitely many-arm bandits. [Carpentier and Valko, ICML 2015]. Finally, we have worked on an efficient sampling of determinantal point processes [Gautier et al, ICML 2016] and applying them to diverse recommendation  and numerical integration.


\citembullet \emph{\bf Semi-supervised apprenticeship learning} with J.\,Audiffren, Mohammad
Ghavamzadeh and Alessandro Lazaric, (2011 - now)\\
In apprenticeship learning we aim to learn a good behavior by observing an
expert or a set of experts. We assume a setting where the expert is maximizing
an unknown true reward function, which is often a linear combination of known
state features. We consider a situation when we observe many trajectories of
behaviors but only one or a few of them are labeled as experts' trajectories. We
investigate the assumptions under which the remaining unlabeled trajectories can
aid in learning a policy with a good performance.


\citembullet \emph{\bf Composing Learning for Artificial Cognitive Systems} with
R\'emi Munos,\\
Mohammad Ghavamzadeh, Alessandro Lazaric, and Daniil Ryabko
 (2011 - 2015) \\
The purpose of this project is to develop a unified toolkit for intelligent
control in many different problem areas. This toolkit will incorporate many of
the most successful approaches to a variety of important control problems within
a single framework, including bandit problems, Markov Decision Processes (MDPs),
Partially Observable MDPs (POMDPs), continuous stochastic control, and
multi-agent systems.



\citembullet \emph{\bf Large-scale semi-supervised learning} with Branislav Kveton, A.\,Saluja
(2010 -  2013) \\
	We parallelized online harmonic solver to process 1 TB of video data in a
day. 	I am working on the multi-manifold learning that can overcome changes in
distribution.
	I am showing how the online learner adapts as to  characters' aging over
10 years period in Married ... with Children sitcom.
 	My research was part of Everyday Sensing and Perception (ESP) project.



\citembullet \emph{\bf Anomaly detection} with Milos Hauskrecht (2007 - 2011)\\
Statistical anomaly detection methods for identification of unusual outcomes and
patient management decisions.
I combined max-margin learning with distance learned to create and anomaly
detector, which outperforms the hospital rule for Heparin Induced
Thrombocytopenia detection.
I later scaled the system for 5K patients with 9K features and 743 clinical
decisions per day.
At the recent study, from 222 alerts 50\%  were highly relevant.

\citembullet \emph{\bf Online semi-supervised learning} with Branislav Kveton
(2009)\\
Extended graph-based semi-supervised learning to the structured case and
demonstrated on handwriting recognition and object detection from video streams.
Regularized harmonic function solution: The algorithm outputs a confidence of
inference and uses it for learning. I came up with an online algorithm that on
the real-world datasets recognizes faces at 80--90\% precision with 90\% recall.

\citembullet
\emph{\bf  Odd-Man-Out} with Wendy Chapman, Roger Day and Gregory Cooper (2007 -
2011)\\
We hypothesized that clinical data in emergency department (ED) reports would
increase sensitivity and specificity of case identification for patients with an
acute lower respiratory syndrome (ALRS). We designed a statistic of disagreement
(odd-man-out) to evalute the machine learning classifier with expert evaluation
in the cases when the gold standard is not available.

\citembullet
\emph{\bf High-throughput proteomic and genomic data and biomarker discovery} with Milos Hauskrecht, Richard Pelikan,   Shuguang Wang
   (2005 - 2007) \\
We built a framework for the cancer prediction from high-throughput proteomic and
genomic data sources.
I found a way to merge heterogeneous data sources: My fusion model was able to
predict
pancreatic cancer from Luminex combined with SELDI with 91.2\% accuracy.

\citembullet
\emph{\bf  Evolutionary feature selection algorithms} with Nuno Marques (2005)\\
I enhanced the existing FeaSANNT neural feature selection with spiking neuron
model to handle inputs noised with up to 10\% Gaussian noise.
\citembullet
\emph{\bf Plastic Synapses} with Juraj Pavlasek (2003 - 2005)\\
I was modelling basic learning function at the level of synapses.  I designed a
model that is able to adapt to the regular frequencies with different a rate as
the time flows. I used genetic programming to find biologically plausible
networks that distinguish different gamma distribution and provided explanation
of the strategies evolved.
\end{category}


%\vskip 1cm

% -------- Reference --------------------------------------------------
\comment{
\begin{category}{References}
  \citemnobullet
  \begin{tabular}{lcl}
  {\bf R\'emi Munos}  & \hspace{2cm} & {\bf Milos Hauskrecht} \\
  Senior Researcher   & & Associate Professor \\
  SequeL team & & University of Pittsburgh \\
  Inria Lille - Nord Europe & & 210 S Bouquet St   \\  
  59650 Villeneuve d'Ascq, France       & & Pittsburgh, PA 15260    \\
  +33 (0)3 59 57 79 06       & &   +1 (412) 624--8845\\
 {\small\tt remi.munos@inria.fr}  & &  {\small\tt milos@pitt.edu} \\
  \\
  \\
   {\bf Gregory F.~Cooper} & \hspace{2cm} & {\bf Roger Day} \\
  Professor & &   Associate  Professor\\
  Department of Biomedical Informatics & & Department of Biostatistics \\
   University of Pittsburgh & &  University of Pittsburgh \\
   200 Meyran Avenue & &  5150 Center Avenue \\
  Pittsburgh, PA 15260 & &   Pittsburgh, PA 15260 \\
   +1 (412) 647--7113 & & +1 (412) 623--3322 \\
   {\small\tt gfc@pitt.edu} & & {\small\tt day01@pitt.edu} \\
%  \end{tabular}
\\
  \\
%  \end{tabular}
%  \citemnobullet
%  \citemnobullet
%  \citemnobullet
%  \begin{tabular}{lcl}
  {\bf Eric P.~Xing} & \hspace{2cm} &   {\bf Branislav Kveton} \\
  Assistant   Professor                  & &   Principal Scientist\\
  ML \&  LTI \& CS Department  & &  Technicolor Labs\\
  Carnegie Mellon University   & &  Technicolor Palo Alto\\
  8101 Gates--Hillman Center  & &  735 Emerson St\\
  Pittsburgh, PA 15213       & &  Palo Alto, CA-94301, USA\\
  +1 (412) 268--2559        & & +1 (408) 653--8607  \\
 {\small\tt epxing@cs.cmu.edu}  & & {\small\tt branislav.kveton@technicolor.com } \\
\\
  \\
  {\bf Jennifer Healey} & \hspace{2cm} &   \\
  Research Scientist                & &  \\
  User Experience Research  & &  \\
  INTEL Research   & &  \\
  2200 Mission College Blvd  & &  \\
 Santa Clara, CA, USA    & &  \\
  +1 (408) 427--7135        & &  \\
 {\small\tt jennifer.healey@intel.com}  & & {\small\tt  } \\
  \end{tabular}

\end{category}
}


\comment{
\begin{category}{References} 
\citembullet Milos Hauskrecht, PhD, Computer Science Department, UPitt, {\small\tt milos\,@\,cs.pitt.edu}
\citembullet Robert P.~Daley, PhD, Computer Science Department, UPitt, {\small\tt daley\,@\,cs.pitt.edu}
\citembullet Gregory F.~Cooper, PhD, MD, Department of Biomedical Informatics, UPitt,  {\small\tt gfc\,@\,pitt.edu}
\citembullet Roger S.~Day, ScD, Department of Biomedical Informatics, UPitt, {\small\tt day\,@\,upci.pitt.edu}
\citembullet Eric P.~Xing, PhD, PhD, Machine Learning \& LTI \& CS Department, CMU, {\small\tt epxing\,@\,cs.cmu.edu}
\end{category}
}
\end{document}

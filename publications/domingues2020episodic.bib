@article{domingues2020episodic,
abstract = {In this paper, we propose new problem-independent lower bounds on the sample complexity and regret in episodic MDPs, with a particular focus on the non-stationary case in which the transition kernel is allowed to change in each stage of the episode. Our main contribution is a novel lower bound of Omega((H3SA/Ïµ2)log(1/$\delta$)) on the sample complexity of an ($\epsilon$,$\delta$)-PAC algorithm for best policy identification in a non-stationary MDP. This lower bound relies on a construction of "hard MDPs" which is different from the ones previously used in the literature. Using this same class of MDPs, we also provide a rigorous proof of the Omega(sqrtH3SAT) regret bound for non-stationary MDPs. Finally, we discuss connections to PAC-MDP lower bounds.},
archivePrefix = {arXiv},
arxivId = {2010.03531},
author = {Domingues, Omar Darwiche and M{\'{e}}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
eprint = {2010.03531},
journal = {Arxiv preprint arXiv:2010.03531},
title = {{Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited}},
url = {https://arxiv.org/abs/2010.03531},
year = {2020}
}
